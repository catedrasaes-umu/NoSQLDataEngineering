Steps:
1. Run locally start.sh
** Logged into container **
2. su - hadoop
3. hdfs dfs -mkdir -p input
4. hdfs dfs -mkdir -p output
5. spark-submit --master yarn --deploy-mode cluster /deployments/es.um.nosql.streaminginference.json2dbschema-0.0.1-SNAPSHOT-jar-with-dependencies.jar --mode file --input input --output output
6. hdfs dfs -put /deployments/movies_1.json input
7. hdfs dfs -cat output/movies.xmi



---- Delete data
hdfs dfs -rm input/*
hdfs dfs -rm output/*

Mongo mode:
----------
1. Run locally start.sh
** Logged into container **
2. su - hadoop
4. hdfs dfs -mkdir -p output
5. spark-submit --master yarn --deploy-mode cluster /deployments/es.um.nosql.streaminginference.json2dbschema-0.0.1-SNAPSHOT-jar-with-dependencies.jar --mode mongo --database test --host mongo --port 27017 --output output
6. Make changes in mongo container: docker exec -it mongodb mongo
7. hdfs dfs -ls output

Couch mode:
----------
1. Run locally start.sh
** Logged into container **
2. su - hadoop
4. hdfs dfs -mkdir -p output
5. spark-submit --master yarn --deploy-mode cluster /deployments/es.um.nosql.streaminginference.json2dbschema-0.0.1-SNAPSHOT-jar-with-dependencies.jar --mode couch --host couch --port 5984 --output output
6. Make changes in http://127.0.0.1:5984/_utils/
7. hdfs dfs -cat output/couch.xmi
